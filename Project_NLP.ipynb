{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYXIHngPoI2W"
      },
      "source": [
        "**Neural Machine Translation with Various Sequence Models**\n",
        "\n",
        "\n",
        "- In this project I have performed Neural Machine Translation with recurrent neural networks and attention based models on Multi30k dataset which include language pairs of German and English.\n",
        "- To this end, you need to implement necessary network components (e.g. LSTMCell, Multi-head attention) using nn.Module class. Then, I experimented those network architectures to get better Bilingual Evaluation Understudy (BLEU) on the test set.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1ZGXqrvlc_O"
      },
      "source": [
        "---\n",
        "# Mounting gdrive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0HEy2Tok-2u"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tsq4xR-liMH"
      },
      "source": [
        "---\n",
        "# Seting up the `root` directory properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHzfVbfmloDz"
      },
      "source": [
        "\n",
        "root = '/gdrive/MyDrive/project_NLP'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ekROPNvqvBr"
      },
      "source": [
        "---\n",
        "#Installing libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpWbZsp2quKI",
        "collapsed": true
      },
      "source": [
        "!pip install torchtext==0.6.0\n",
        "!pip install spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XDVC3lJl1_H"
      },
      "source": [
        "---\n",
        "# Basic settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jIiyjEGlwJ8"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGlC_yM9lvue"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import SGD\n",
        "import torchtext\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext import data\n",
        "from torchtext.data.metrics import bleu_score\n",
        "import spacy\n",
        "from spacy.symbols import ORTH\n",
        "import math\n",
        "import random\n",
        "import tqdm.notebook as tq\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W27cdKyEmJy1"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dczwhJI1l8Kl"
      },
      "source": [
        "# Basic settings\n",
        "torch.manual_seed(470)\n",
        "torch.cuda.manual_seed(470)\n",
        "\n",
        "#!pip install easydict\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "args = edict()\n",
        "args.batch_size = 32\n",
        "args.nlayers = 2\n",
        "args.ninp = 256\n",
        "args.nhid = 256 #512\n",
        "\n",
        "\n",
        "args.clip = 1\n",
        "args.lr_lstm = 0.001\n",
        "args.dropout = 0.2\n",
        "args.nhid_attn = 256\n",
        "args.epochs = 20\n",
        "\n",
        "##### Transformer\n",
        "args.nhid_tran = 256\n",
        "args.nhead = 8\n",
        "args.nlayers_transformer = 6\n",
        "args.attn_pdrop = 0.1\n",
        "args.resid_pdrop = 0.1\n",
        "args.embd_pdrop = 0.1\n",
        "args.nff = 4 * args.nhid_tran\n",
        "\n",
        "\n",
        "args.lr_transformer = 0.0001 #1.0\n",
        "args.betas = (0.9, 0.98)\n",
        "\n",
        "args.gpu = True\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
        "\n",
        "result_dir = Path(root) / 'results'\n",
        "result_dir.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQZvsEeDmeZt"
      },
      "source": [
        "---\n",
        "# Utility functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUOvr2A_mf4Q"
      },
      "source": [
        "def word_ids_to_sentence(id_tensor, vocab, join=' '):\n",
        "    \"\"\"Converts a sequence of word ids to a sentence\"\"\"\n",
        "    if isinstance(id_tensor, torch.LongTensor):\n",
        "        ids = id_tensor.transpose(0, 1).contiguous().view(-1)\n",
        "    elif isinstance(id_tensor, np.ndarray):\n",
        "        ids = id_tensor.transpose().reshape(-1)\n",
        "    batch = [vocab.itos[ind] for ind in ids] # denumericalize\n",
        "    if join is None:\n",
        "        return batch\n",
        "    else:\n",
        "        return join.join(batch)\n",
        "\n",
        "# Extracts bias and non-bias parameters from a model.\n",
        "def get_parameters(model, bias=False):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            if bias:\n",
        "                yield m.bias\n",
        "            else:\n",
        "                yield m.weight\n",
        "        else:\n",
        "            if not bias:\n",
        "                yield m.parameters()\n",
        "\n",
        "def run_epoch(epoch, model, optimizer, is_train=True, data_iter=None):\n",
        "    total_loss = 0\n",
        "    n_correct = 0\n",
        "    n_total = 0\n",
        "    if data_iter is None:\n",
        "        data_iter = train_iter if is_train else valid_iter\n",
        "    if is_train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for batch in data_iter:\n",
        "        x, y, length = sort_batch(batch.src.to(device), batch.trg.to(device))\n",
        "        target = y[1:]\n",
        "        if isinstance(model, Transformer):\n",
        "            x, y = x.transpose(0, 1), y.transpose(0, 1)\n",
        "            target = target.transpose(0, 1) #y[:, 1:]\n",
        "        pred = model(x, y, length)\n",
        "        loss = criterion(pred.reshape(-1, trg_ntoken), target.reshape(-1))\n",
        "        n_targets = (target != pad_id).long().sum().item()\n",
        "        n_total += n_targets\n",
        "        n_correct += (pred.argmax(-1) == target)[target != pad_id].long().sum().item()\n",
        "        if is_train:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        total_loss += loss.item() * n_targets\n",
        "    total_loss /= n_total\n",
        "    print(\"Epoch\", epoch, 'Train' if is_train else 'Valid',\n",
        "          \"Loss\", np.mean(total_loss),\n",
        "          \"Acc\", n_correct / n_total,\n",
        "          \"PPL\", np.exp(total_loss))\n",
        "    return total_loss\n",
        "\n",
        "def word_ids_to_sentence_(ids, vocab):\n",
        "    sentence = []\n",
        "    for ind in ids:\n",
        "        if ind == eos_id:\n",
        "            break\n",
        "        sentence.append(vocab.itos[ind])\n",
        "    return sentence\n",
        "\n",
        "def run_translation(model, data_iter, max_len=100, mode='best'):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        load_model(model, mode)\n",
        "        src_list = []\n",
        "        gt_list = []\n",
        "        pred_list = []\n",
        "        for batch in data_iter:\n",
        "            x, y, length = sort_batch(batch.src.to(device), batch.trg.to(device))\n",
        "            target = y[1:]\n",
        "            if isinstance(model, Transformer):\n",
        "                x, y = x.transpose(0, 1), y.transpose(0, 1)\n",
        "                target = target.transpose(0, 1)\n",
        "            pred = model(x, y, length, max_len=max_len, teacher_forcing=False)\n",
        "            pred_token = pred.argmax(-1)\n",
        "            if not isinstance(model, Transformer):\n",
        "                pred_token = pred_token.transpose(0, 1).cpu().numpy()\n",
        "                y = y.transpose(0, 1).cpu().numpy()\n",
        "                x = x.transpose(0, 1).cpu().numpy()\n",
        "            # pred_token : batch_size x max_len\n",
        "            for x_, y_, pred_ in zip(x, y, pred_token):\n",
        "                src_list.append(word_ids_to_sentence_(x_[1:], SRC.vocab))\n",
        "                gt_list.append([word_ids_to_sentence_(y_[1:], TRG.vocab)])\n",
        "                pred_list.append(word_ids_to_sentence_(pred_, TRG.vocab))\n",
        "\n",
        "        for i in range(5):\n",
        "            print(f\"--------- Translation Example {i+1} ---------\")\n",
        "            print(\"SRC :\", ' '.join(src_list[i]))\n",
        "            print(\"TRG :\", ' '.join(gt_list[i][0]))\n",
        "            print(\"PRED:\", ' '.join(pred_list[i]))\n",
        "        print()\n",
        "        print(\"BLEU:\", bleu_score(pred_list, gt_list))\n",
        "\n",
        "\n",
        "\n",
        "def save_model(model, mode=\"last\"):\n",
        "    torch.save(model.state_dict(),  result_dir / f'{type(model).__name__}_{mode}.ckpt')\n",
        "\n",
        "def load_model(model, mode=\"last\"):\n",
        "    if os.path.exists(result_dir / f'{type(model).__name__}_{mode}.ckpt'):\n",
        "        model.load_state_dict(torch.load(result_dir / f'{type(model).__name__}_{mode}.ckpt'))\n",
        "\n",
        "def sort_batch(X, y, lengths=None):\n",
        "    if lengths is None:\n",
        "        lengths = (X != pad_id_src).long().sum(0)\n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = torch.index_select(X, 1, indx)\n",
        "    y = torch.index_select(y, 1, indx)\n",
        "    return X, y, lengths\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7WMaIEVmoub"
      },
      "source": [
        "---\n",
        "# Define `DataLoader` for training & validation set\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually fix Multi30K download link, since the original server is down. (https://github.com/pytorch/text/issues/1756)\n",
        "Multi30k.urls = [\n",
        "    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\",\n",
        "    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\",\n",
        "    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\",\n",
        "]"
      ],
      "metadata": {
        "id": "1HFB7-CvgFYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deEP6JnaywMV"
      },
      "source": [
        "SRC = Field(tokenize = \"spacy\",\n",
        "            tokenizer_language=\"de_core_news_sm\",\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = \"spacy\",\n",
        "            tokenizer_language=\"en_core_web_sm\",\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
        "                                                    fields = (SRC, TRG), test='test')\n",
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)\n",
        "\n",
        "src_ntoken = len(SRC.vocab.stoi)\n",
        "trg_ntoken = len(TRG.vocab.stoi)\n",
        "\n",
        "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = args.batch_size,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSAnejfCywMZ",
        "collapsed": true
      },
      "source": [
        "pad_id_trg = TRG.vocab.stoi[TRG.pad_token]\n",
        "pad_id_src = SRC.vocab.stoi[SRC.pad_token]\n",
        "pad_id = pad_id_src\n",
        "eos_id = TRG.vocab.stoi[TRG.eos_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "\n",
        "for batch in train_iter:\n",
        "    src, trg, length_src = sort_batch(batch.src, batch.trg)\n",
        "    print(length_src)\n",
        "    print(src, src.shape)\n",
        "    print(trg, trg.shape)\n",
        "    break\n",
        "\n",
        "print(\"##### EXAMPLE #####\")\n",
        "print(\"SRC: \", word_ids_to_sentence(src[:, 1:2].long().cpu(), SRC.vocab))\n",
        "print(\"TRG: \", word_ids_to_sentence(trg[:, 1:2].long().cpu(), TRG.vocab))\n",
        "\n",
        "print(\"SRC vocab size\", len(SRC.vocab.stoi))\n",
        "print(\"TRG vocab size\", len(TRG.vocab.stoi))\n",
        "print(\"Vocab\", list(SRC.vocab.stoi.items())[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVy7Dc_rmw2A"
      },
      "source": [
        "---\n",
        "#  networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yg7ZEwvO9_9"
      },
      "source": [
        "##  Implementing LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YK28D7EO_Fd"
      },
      "source": [
        "### (a)  Implementing LSTMCell\n",
        "- LSTMCell is a single unit constructing LSTM. It gets current input(`x`) and previous state (which is composed of hidden state `hx` and cell state `cx`) as inputs and returns the state for the next time step (`hy` and `cy`). There are four switch variables to handle information flows through time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT0CJePPPS_E"
      },
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.linear_input = nn.Linear(input_size, 4 * hidden_size)\n",
        "        self.linear_hidden = nn.Linear(hidden_size, 4 * hidden_size)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "\n",
        "        hx, cx = state\n",
        "        gates = self.linear_input(x) + self.linear_hidden(hx)\n",
        "\n",
        "\n",
        "        f, i, o, g = torch.chunk(gates, chunks=4, dim=1)\n",
        "\n",
        "\n",
        "        f = torch.sigmoid(f)\n",
        "        i = torch.sigmoid(i)\n",
        "        o = torch.sigmoid(o)\n",
        "        g = torch.tanh(g)\n",
        "\n",
        "        # Update cell state\n",
        "        cy = f * cx + i * g\n",
        "\n",
        "        # Update hidden state\n",
        "        hy = o * torch.tanh(cy)\n",
        "\n",
        "        # Return the new state\n",
        "        return hy, (hy,cy)\n",
        "\n",
        "        ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOd-XpyGO_qH"
      },
      "source": [
        "### (b)  Implementing LSTM\n",
        "- LSTMLayer is a single layer composed of sequential LSTMCells. While LSTMCell handles a single input, LSTMLayer gets a sequence as an input and processes it in an autoregressive manner.\n",
        " `states` now contain multiple `state`s where each state becomes an initial state for a different level of LSTMLayers. Also, each output of an LSTMLayer is fed into the next layer of LSTMLayer as an input.\n",
        "As a result, LSTM returns `output` tensor of size (L,B,nhid) and `output_states` consists of output states from different levels of LSTMLayers, which is a type of List(Tensor, Tensor, ..., Tensor) and each Tensor has a size of (L,B,nhid). Here L,B,nhid are a maximum length of sentences within a batch (equal to `x.size(0)`), batch size, and dimension size of hidden states, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjf1QmRAPYdd"
      },
      "source": [
        "class LSTMLayer(nn.Module):\n",
        "    def __init__(self,*cell_args):\n",
        "        super(LSTMLayer, self).__init__()\n",
        "        self.cell = LSTMCell(*cell_args)\n",
        "\n",
        "    def forward(self, x, state, length_x=None):\n",
        "\n",
        "        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]\n",
        "        inputs = x.unbind(0)\n",
        "        assert (length_x is None) or torch.all(length_x == length_x.sort(descending=True)[0])\n",
        "        outputs = []\n",
        "        out_hidden_state = []\n",
        "        out_cell_state = []\n",
        "        for i in range(len(inputs)):\n",
        "            out, state = self.cell(inputs[i] , state)\n",
        "            outputs += [out]\n",
        "            if length_x is not None:\n",
        "                if torch.any(i+1 == length_x):\n",
        "                    out_hidden_state = [state[0][i+1==length_x]] + out_hidden_state\n",
        "                    out_cell_state = [state[1][i+1==length_x]] + out_cell_state\n",
        "        if length_x is not None:\n",
        "            state = (torch.cat(out_hidden_state, dim=0), torch.cat(out_cell_state, dim=0))\n",
        "        return torch.stack(outputs), state\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, ninp, nhid, nlayers, dropout):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.layers = []\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        for i in range(nlayers):\n",
        "            if i == 0:\n",
        "                self.layers.append(LSTMLayer(ninp, nhid))\n",
        "            else:\n",
        "                self.layers.append(LSTMLayer(nhid, nhid))\n",
        "        self.layers = nn.ModuleList(self.layers)\n",
        "\n",
        "    def forward(self, x, states, length_x=None):\n",
        "\n",
        "        output_states = []\n",
        "        output = x\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "\n",
        "            output, state = layer(output, states[i], length_x=length_x)\n",
        "            output_states.append(state)\n",
        "\n",
        "\n",
        "            if i < len(self.layers) - 1:\n",
        "                output = self.dropout(output)\n",
        "\n",
        "\n",
        "        return output, output_states\n",
        "        ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXJsTGg3O-m4"
      },
      "source": [
        "### (c)  Implementing LSTMEncoder\n",
        "LSTMEncoder encodes a sequence of tokens into the context vector. It first embeds a tokenized sequence using the embedding layer followed by dropout layer, and then LSTM computes `output` and `context_vector`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b6J1a39PY4P"
      },
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        ninp = args.ninp\n",
        "        nhid = args.nhid\n",
        "        nlayers = args.nlayers\n",
        "        dropout = args.dropout\n",
        "        self.embed = nn.Embedding(src_ntoken, ninp, padding_idx=pad_id)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = LSTM(ninp, nhid, nlayers, dropout)\n",
        "\n",
        "    def forward(self, x, states, length_x=None):\n",
        "\n",
        "\n",
        "        embedded = self.embed(x)\n",
        "\n",
        "\n",
        "        dropped = self.dropout(embedded)\n",
        "\n",
        "\n",
        "        output, context_vector = self.lstm(dropped, states, length_x)\n",
        "\n",
        "        return output, context_vector\n",
        "        # ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrFAmuJRO9eG"
      },
      "source": [
        "### (d)  Implementing LSTMDecoder\n",
        "LSTMDecoder gets a single token as an input to predict the next token. Similar to LSTMEncoder, it first embeds a given input (usually a predicted token from last time step) using embedding layer followed by dropout layer, and then LSTM computes `output` and `output_states`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2vRxaa3PZom"
      },
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.embed = nn.Embedding(trg_ntoken, args.ninp, padding_idx=pad_id)\n",
        "        self.lstm = LSTM(args.ninp, args.nhid, args.nlayers, args.dropout)\n",
        "        self.fc_out = nn.Linear(args.nhid, trg_ntoken)\n",
        "        self.dropout = nn.Dropout(args.dropout)\n",
        "        self.fc_out.weight = self.embed.weight\n",
        "\n",
        "    def forward(self, x, states):\n",
        "\n",
        "\n",
        "        embedded = self.embed(x)\n",
        "\n",
        "\n",
        "        dropped = self.dropout(embedded)\n",
        "\n",
        "\n",
        "        lstm_output, output_states = self.lstm(dropped, states)\n",
        "\n",
        "\n",
        "        output = self.fc_out(lstm_output)\n",
        "\n",
        "        return output, output_states\n",
        "        ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjbbVJcbO9I3"
      },
      "source": [
        "### (e)  Implementing LSTMSeq2Seq\n",
        "LSTMSeq2Seq is a complete model for neural machine translation. It starts with LSTMEncoder encoding a given tokenized sequence into the context vector. LSMTDecoder then decodes the context vector step by step. As mentioned in the description for LSTMDecoder, each input for the decoder is a token predicted by the previous decoder. In the training stage, however, one noisy prediction from the previous decoder can mess up all of the following predictions so teacher forcing is used in the training stage. Teacher forcing allows LSTMdecoder to always ground-truth token as an input instead of predicted one from the previous step. For LSTMDecoder if `teacher_focing` is True (it's the case for training stage), and use the predicted token from last time step otherwise (case for inference). All of the sentences start with <sos> token so the first input token to LSTMDecoder should be always `<sos>`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLC1mpEFm05u"
      },
      "source": [
        "class LSTMSeq2Seq(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMSeq2Seq, self).__init__()\n",
        "        self.encoder = LSTMEncoder()\n",
        "        self.decoder = LSTMDecoder()\n",
        "\n",
        "    def _get_init_states(self, x):\n",
        "        init_states = [\n",
        "            (torch.zeros((x.size(1), args.nhid)).to(x.device),\n",
        "            torch.zeros((x.size(1), args.nhid)).to(x.device))\n",
        "            for _ in range(args.nlayers)\n",
        "        ]\n",
        "        return init_states\n",
        "\n",
        "    def forward(self, x, y, length, max_len=None, teacher_forcing=True):\n",
        "\n",
        "\n",
        "        init_states = [\n",
        "            (\n",
        "                torch.zeros(x.size(1), self.encoder.lstm.layers[0].cell.hidden_size, device=x.device),\n",
        "                torch.zeros(x.size(1), self.encoder.lstm.layers[0].cell.hidden_size, device=x.device)\n",
        "            )\n",
        "            for _ in range(len(self.encoder.lstm.layers))\n",
        "        ]\n",
        "\n",
        "        # Step 2: Pass through the encoder\n",
        "        _, output_states = self.encoder(x, init_states, length)\n",
        "\n",
        "        # Step 3: Prepare decoding variables\n",
        "        trg_len = y.size(0) if max_len is None else max_len\n",
        "        batch_size = y.size(1)\n",
        "        trg_ntoken = self.decoder.fc_out.out_features\n",
        "        outputs = torch.zeros(trg_len - 1, batch_size, trg_ntoken, device=x.device)\n",
        "\n",
        "\n",
        "        dec_input = y[0:1]  # Shape: (1, B)\n",
        "\n",
        "        #\n",
        "        dec_states = output_states\n",
        "        for t in range(1, trg_len):\n",
        "            dec_output, dec_states = self.decoder(dec_input, dec_states)\n",
        "            outputs[t - 1] = dec_output\n",
        "\n",
        "\n",
        "            if teacher_forcing:\n",
        "                dec_input = y[t:t + 1]  #\n",
        "            else:\n",
        "                dec_input = dec_output.argmax(-1)  #\n",
        "        return outputs\n",
        "        ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8wX7tHCywMf"
      },
      "source": [
        "## Implementing LSTM  with Attention\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wbnEENVFXuy"
      },
      "source": [
        "### (a) Implementing Attention\n",
        "Here, I implemented an attention module to augment vanilla LSTM. This attention module (also known as the Bahdanau attention, or the Additive attention) will combine the output of LSTMEncoder and the current decoder's state input to determine which part of the encoder output to focus on this particular time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUDUEcWwrYyk"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.nhid_enc = args.nhid\n",
        "        self.nhid_dec = args.nhid\n",
        "        self.W1 = nn.Linear(self.nhid_enc, args.nhid_attn)\n",
        "        self.W2 = nn.Linear(self.nhid_dec, args.nhid_attn)\n",
        "        self.W3 = nn.Linear(args.nhid_attn, 1)\n",
        "\n",
        "    def forward(self, x, enc_o, dec_h, length_enc=None):\n",
        "\n",
        "\n",
        "        L, B, _ = enc_o.size()  #\n",
        "        enc_proj = self.W1(enc_o)  #\n",
        "        dec_proj = self.W2(dec_h).unsqueeze(0)  #\n",
        "\n",
        "        scores = self.W3(torch.tanh(enc_proj + dec_proj)).squeeze(-1)  #\n",
        "\n",
        "        #\n",
        "        if length_enc is not None:\n",
        "            mask = torch.arange(L, device=enc_o.device).unsqueeze(1) >= length_enc.unsqueeze(0)\n",
        "            scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        #\n",
        "        attn_weights = F.softmax(scores, dim=0)  # Shape: (L, B)\n",
        "\n",
        "        #\n",
        "        context = torch.sum(attn_weights.unsqueeze(-1) * enc_o, dim=0, keepdim=True)  # Shape: (1, B, nhid_enc)\n",
        "\n",
        "        return context\n",
        "\n",
        "        ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqdGZ8JMrVQ_"
      },
      "source": [
        "### (b) Implementing LSTMAttnDecoder\n",
        "LSTMAttnDecoder is an extension of LSTMDecoder from above, with an extra attention layer. The difference is that the attention values are computed first then passed to the decoder, instead of directly feeding the previous decoder step's output to the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsWZTFDMrexf"
      },
      "source": [
        "class LSTMAttnDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMAttnDecoder, self).__init__()\n",
        "        self.embed = nn.Embedding(trg_ntoken, args.ninp, padding_idx=pad_id)\n",
        "        self.lstm = LSTM(args.ninp + args.nhid, args.nhid, args.nlayers, args.dropout)\n",
        "        self.fc_out = nn.Linear(args.nhid, trg_ntoken)\n",
        "        self.dropout = nn.Dropout(args.dropout)\n",
        "        self.attn = Attention()\n",
        "        self.fc_out.weight = self.embed.weight\n",
        "\n",
        "    def forward(self, x, enc_o, states, length_enc=None):\n",
        "\n",
        "\n",
        "        embedded = self.embed(x)  # Shape: (1, B, ninp)\n",
        "        dropped = self.dropout(embedded)  # Apply dropout, Shape: (1, B, ninp)\n",
        "\n",
        "\n",
        "        context = self.attn(dropped, enc_o, states[0][0], length_enc)\n",
        "\n",
        "\n",
        "        lstm_input = torch.cat([dropped, context], dim=-1)\n",
        "\n",
        "\n",
        "        dec_output, output_states = self.lstm(lstm_input, states)\n",
        "\n",
        "\n",
        "        dec_output = self.fc_out(dec_output)\n",
        "\n",
        "        return dec_output, output_states\n",
        "        ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7tmYYFPrWp3"
      },
      "source": [
        "### (c) Implementing LSTMAttnSeq2Seq\n",
        "LSTMAttnSeq2Seq is a complete model for neural machine translation, with an attention layer added. The encoder part of this section is identical to the previous LSTMSeq2Seq model.Teacher forcing is used in the training stage to mitigate noise prediction. However, the decoder part has changed to LSTMAttnDecoder module. I modified the previous implementation of LSTMSeq2Seq model accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R3nNAfJywMf"
      },
      "source": [
        "class LSTMAttnSeq2Seq(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMAttnSeq2Seq, self).__init__()\n",
        "        self.encoder = LSTMEncoder()\n",
        "        self.decoder = LSTMAttnDecoder()\n",
        "\n",
        "    def _get_init_states(self, x):\n",
        "        init_states = [\n",
        "            (torch.zeros((x.size(1), args.nhid)).to(x.device),\n",
        "            torch.zeros((x.size(1), args.nhid)).to(x.device))\n",
        "            for _ in range(args.nlayers)\n",
        "        ]\n",
        "        return init_states\n",
        "\n",
        "    def forward(self, x, y, length, max_len=None, teacher_forcing=True):\n",
        "\n",
        "\n",
        "        batch_size = x.size(1)\n",
        "        nhid = self.encoder.lstm.layers[0].cell.hidden_size\n",
        "        num_layers = len(self.encoder.lstm.layers)\n",
        "        device = x.device\n",
        "\n",
        "        init_states = [\n",
        "            (\n",
        "                torch.zeros(batch_size, nhid, device=device),\n",
        "                torch.zeros(batch_size, nhid, device=device),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        #\n",
        "        enc_output, enc_states = self.encoder(x, init_states, length)\n",
        "\n",
        "        #\n",
        "        trg_len = y.size(0) if max_len is None else max_len\n",
        "        trg_ntoken = self.decoder.fc_out.out_features\n",
        "        outputs = torch.zeros(trg_len - 1, batch_size, trg_ntoken, device=device)\n",
        "\n",
        "        #\n",
        "        dec_input = y[0:1]  # Shape: (1, B)\n",
        "\n",
        "        #\n",
        "        dec_states = enc_states  #\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            dec_output, dec_states = self.decoder(dec_input, enc_output, dec_states, length)\n",
        "            outputs[t - 1] = dec_output\n",
        "\n",
        "            if teacher_forcing:\n",
        "                dec_input = y[t:t + 1]\n",
        "            else:\n",
        "                dec_input = dec_output.argmax(-1)\n",
        "\n",
        "        return outputs\n",
        "        ################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG_fK-sWjLT5"
      },
      "source": [
        "\n",
        "##  Implementing Transformer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4tGUyntXKXk"
      },
      "source": [
        "### (a) Implementing MaskedMultiheadAttention\n",
        "In this module, I implement a single layer of multi-head attention, which will be the key building block of the Transformer model. Each query, key, value input will first pass through a feed-forward network, then scaled dot-product attention is performed. Additionally, there's an optional mask layer inside the scaled dot-product attention applied only in the decoder stage of the Transformer, to prevent the model from being able to see future inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWn3GenhPG1G"
      },
      "source": [
        "\n",
        "### (b) Implementing TransformerEncLayer\n",
        "This module is a single layer of the Transformer encoder, containing a layer of masked multi-head attention and a feed-forward network with dropout and skip connection. Both attention and feed-forward layer have skip connections and are preceded by LayerNorm. I stack this layer multiple times to create the full version of the encoder. Since attention is performed in a self-attention manner, I pass the same values to query, key, and value inputs of the MaskedSelfAttention module.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFeZAJNCPEll"
      },
      "source": [
        "MAX_LEN = 100\n",
        "class MaskedMultiheadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked attention layer with a projection at the end.\n",
        "    \"\"\"\n",
        "    def __init__(self, mask=False):\n",
        "        super(MaskedMultiheadAttention, self).__init__()\n",
        "        assert args.nhid_tran % args.nhead == 0\n",
        "        # mask : whether to use\n",
        "        # key, query, value projections for all heads\n",
        "        self.key = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
        "        self.query = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
        "        self.value = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(args.attn_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        if mask:\n",
        "            self.register_buffer(\"mask\", torch.tril(torch.ones(MAX_LEN, MAX_LEN)))\n",
        "        self.nhead = args.nhead\n",
        "        self.d_k = args.nhid_tran // args.nhead\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "        B, T_q, _ = q.size()\n",
        "        _, T, _ = k.size()\n",
        "\n",
        "\n",
        "        q_proj = self.query(q).view(B, T_q, self.nhead, self.d_k).transpose(1, 2)\n",
        "        k_proj = self.key(k).view(B, T, self.nhead, self.d_k).transpose(1, 2)\n",
        "        v_proj = self.value(v).view(B, T, self.nhead, self.d_k).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q_proj, k_proj.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "\n",
        "        if mask is not None:\n",
        "\n",
        "            if mask.dim() == 2:\n",
        "                mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "            elif mask.dim() == 3:\n",
        "                mask = mask.unsqueeze(1)\n",
        "\n",
        "\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.attn_drop(attn_weights)\n",
        "        attn_output = torch.matmul(attn_weights, v_proj)\n",
        "\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T_q, -1)\n",
        "        outputs = self.proj(attn_output)\n",
        "\n",
        "        return outputs\n",
        "        ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RcLE8QKPPfJ"
      },
      "source": [
        "class TransformerEncLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerEncLayer, self).__init__()\n",
        "        self.ln1 = nn.LayerNorm(args.nhid_tran)\n",
        "        self.ln2 = nn.LayerNorm(args.nhid_tran)\n",
        "        self.attn = MaskedMultiheadAttention()\n",
        "        self.dropout1 = nn.Dropout(args.resid_pdrop)\n",
        "        self.dropout2 = nn.Dropout(args.resid_pdrop)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(args.nhid_tran, args.nff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(args.nff, args.nhid_tran)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "\n",
        "\n",
        "        x_norm = self.ln1(x)\n",
        "        attn_output = self.attn(x_norm, x_norm, x_norm, mask)\n",
        "\n",
        "\n",
        "        x = x + self.dropout1(attn_output)\n",
        "\n",
        "\n",
        "        x_norm = self.ln2(x)\n",
        "        ff_output = self.ff(x_norm)\n",
        "\n",
        "\n",
        "        x = x + self.dropout2(ff_output)\n",
        "\n",
        "        return x\n",
        "        ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKz8TOrEPVu4"
      },
      "source": [
        "### (c) ImplementING TransformerDecLayer\n",
        "This module is a single layer of the Transformer decoder. The module contains two masked multi-head attentions and a feed-forward network, all with a skip connection and a preceding LayerNorm. The first attention is identical to the encoder's attention. However, the second attention is a cross-attention: that is, the key and value inputs of this layer would be the encoded words from the **source** sentence, given as `enc_o`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnp9dhwsPXjB"
      },
      "source": [
        "class TransformerDecLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerDecLayer, self).__init__()\n",
        "        self.ln1 = nn.LayerNorm(args.nhid_tran)\n",
        "        self.ln2 = nn.LayerNorm(args.nhid_tran)\n",
        "        self.ln3 = nn.LayerNorm(args.nhid_tran)\n",
        "        self.dropout1 = nn.Dropout(args.resid_pdrop)\n",
        "        self.dropout2 = nn.Dropout(args.resid_pdrop)\n",
        "        self.dropout3 = nn.Dropout(args.resid_pdrop)\n",
        "        self.attn1 = MaskedMultiheadAttention(mask=True) # self-attention\n",
        "        self.attn2 = MaskedMultiheadAttention() # tgt to src attention\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(args.nhid_tran, args.nff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(args.nff, args.nhid_tran)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, enc_o, enc_mask=None):\n",
        "\n",
        "\n",
        "        x_norm = self.ln1(x)\n",
        "        dec_attn_output = self.attn1(x_norm, x_norm, x_norm)\n",
        "        dec_attn_output = self.dropout1(dec_attn_output)\n",
        "        x = x + dec_attn_output\n",
        "\n",
        "\n",
        "        x_norm = self.ln2(x)\n",
        "        enc_dec_attn_output = self.attn2(x_norm, enc_o, enc_o, enc_mask)\n",
        "        enc_dec_attn_output = self.dropout2(enc_dec_attn_output)\n",
        "        x = x + enc_dec_attn_output\n",
        "\n",
        "        x_norm = self.ln3(x)\n",
        "        ff_output = self.ff(x_norm)\n",
        "        ff_output = self.dropout3(ff_output)\n",
        "        x = x + ff_output\n",
        "\n",
        "        return x\n",
        "\n",
        "        ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO7tjDWvPbx3"
      },
      "source": [
        "### (d) Implementing TransformerEncoder\n",
        "In this module, I first tokenize the input word, apply positional encoding, then pass through multiple layers of TransformerEncLayer, and conclude with a LayerNorm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjdoK0opPdEC"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len=4096):\n",
        "        super().__init__()\n",
        "        dim = args.nhid_tran\n",
        "        pos = np.arange(0, max_len)[:, None]\n",
        "        i = np.arange(0, dim // 2)\n",
        "        denom = 10000 ** (2 * i / dim)\n",
        "\n",
        "        pe = np.zeros([max_len, dim])\n",
        "        pe[:, 0::2] = np.sin(pos / denom)\n",
        "        pe[:, 1::2] = np.cos(pos / denom)\n",
        "        pe = torch.from_numpy(pe).float()\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return x + self.pe[:x.shape[1]]\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(src_ntoken, args.nhid_tran)\n",
        "        self.pos_enc = PositionalEncoding()\n",
        "        self.dropout = nn.Dropout(args.embd_pdrop)\n",
        "        # transformer\n",
        "        self.transform = nn.ModuleList([TransformerEncLayer() for _ in range(args.nlayers_transformer)])\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(args.nhid_tran)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "\n",
        "\n",
        "        x = self.tok_emb(x)\n",
        "        x = self.pos_enc(x)\n",
        "\n",
        "\n",
        "        x = self.dropout(x)  #\n",
        "\n",
        "\n",
        "        for layer in self.transform:  #\n",
        "            x = layer(x, mask)  #\n",
        "\n",
        "\n",
        "        outputs = self.ln_f(x)\n",
        "\n",
        "        return outputs\n",
        "        ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkSdxiGJPgPm"
      },
      "source": [
        "### (e) Implementing TransformerDecoder\n",
        "What TransformerDecoder does is pretty much identical to TransformerEncoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft64IQHrPlCb"
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.tok_emb = nn.Embedding(trg_ntoken, args.nhid_tran)\n",
        "        self.pos_enc = PositionalEncoding()\n",
        "        self.dropout = nn.Dropout(args.embd_pdrop)\n",
        "        self.transform = nn.ModuleList([TransformerDecLayer() for _ in range(args.nlayers_transformer)])\n",
        "        self.ln_f = nn.LayerNorm(args.nhid_tran)\n",
        "        self.lin_out = nn.Linear(args.nhid_tran, trg_ntoken)\n",
        "        self.lin_out.weight = self.tok_emb.weight\n",
        "\n",
        "\n",
        "    def forward(self, x, enc_o, enc_mask):\n",
        "\n",
        "\n",
        "        x = self.tok_emb(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "\n",
        "        for layer in self.transform:\n",
        "            x = layer(x, enc_o, enc_mask)\n",
        "\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "        x = x * (1 / (self.tok_emb.embedding_dim ** 0.5))\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nxKg_RfPpz4"
      },
      "source": [
        "### (f) Implementing Transformer\n",
        "Finally, let's combine everything to construct the full Transformer model. By creating a mask according to `length_x` parameter, and pass the inputs through TransformerEncoder to obtain encoder output. Now if on training mode (`self.training == True`) or teacher forcing is enabled, then run through the decoder exactly once to predict the very next word. Otherwise, run through the decoder `max_len - 1` times to create a sequence of `max_len` tokens. The first token to feed the decoder is always the first token of `y`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWsZrYEGywMi"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = TransformerEncoder()\n",
        "        self.decoder = TransformerDecoder()\n",
        "\n",
        "    def forward(self, x, y, length_x, max_len=None, teacher_forcing=True):\n",
        "\n",
        "\n",
        "        enc_mask = (torch.arange(x.size(1)).expand(len(length_x), -1).to(x.device) < length_x.unsqueeze(1))\n",
        "        enc_o = self.encoder(x, mask=enc_mask)\n",
        "\n",
        "\n",
        "        if max_len is None:\n",
        "            max_len = y.size(1)\n",
        "\n",
        "\n",
        "        if teacher_forcing and self.training:\n",
        "\n",
        "            y = y[:, :-1]\n",
        "\n",
        "            outputs = self.decoder(y, enc_o, enc_mask)\n",
        "\n",
        "        else:\n",
        "\n",
        "            B = y.size(0)\n",
        "            trg_ntoken = self.decoder.lin_out.out_features\n",
        "\n",
        "\n",
        "            dec_input = y[:, :1]  # Shape: (B, 1)\n",
        "            outputs = []\n",
        "\n",
        "            for t in range(max_len - 1):\n",
        "\n",
        "                dec_output = self.decoder(dec_input, enc_o, enc_mask)  # Shape: (B, t+1, trg_ntoken)\n",
        "                dec_output_t = dec_output[:, -1:]  # Extract the last timestep output, shape: (B, 1, trg_ntoken)\n",
        "\n",
        "\n",
        "                outputs.append(dec_output_t)\n",
        "\n",
        "\n",
        "                next_token = dec_output_t.argmax(-1)  # Get predicted token, shape: (B, 1)\n",
        "                dec_input = torch.cat([dec_input, next_token], dim=1)  # Append predicted token\n",
        "\n",
        "\n",
        "            outputs = torch.cat(outputs, dim=1)  # Shape: (B, max_len-1, trg_ntoken)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "        ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHQzs6ocPxtQ"
      },
      "source": [
        "## Runing Experiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdxUX81cywMk"
      },
      "source": [
        "def run_experiment(model):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr_lstm if not isinstance(model, Transformer) else args.lr_transformer)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "            factor=0.25, patience=1, threshold=0.0001, threshold_mode='rel',\n",
        "            cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
        "\n",
        "    best_val_loss = np.inf\n",
        "    for epoch in tq.tqdm(range(args.epochs)):\n",
        "        run_epoch(epoch, model, optimizer, is_train=True)\n",
        "        with torch.no_grad():\n",
        "            val_loss = run_epoch(epoch, model, None, is_train=False)\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            save_model(model, 'best')\n",
        "        save_model(model)\n",
        "        scheduler.step(val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UytcJfBv8b3Y"
      },
      "source": [
        "## Training and Validating models\n",
        "\n",
        "**Obtained BLEU Score**\n",
        "- LSTMSeq2Seq: 0.05169190386007824\n",
        "- LSTMAttnSeq2Seq: 0.05828504336025121\n",
        "- Transformer: 0.35190386023641020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ByvNFinywMm"
      },
      "source": [
        "lstm_model = LSTMSeq2Seq().to(device)\n",
        "lstm_model.apply(init_weights)\n",
        "run_experiment(lstm_model)\n",
        "run_translation(lstm_model, test_iter, max_len=100)\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRupiwcWCAaO"
      },
      "source": [
        "attn_model = LSTMAttnSeq2Seq().to(device)\n",
        "attn_model.apply(init_weights)\n",
        "run_experiment(attn_model)\n",
        "run_translation(attn_model, test_iter, max_len=100)\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU65AMeUywMu"
      },
      "source": [
        "transformer_model = Transformer().to(device)\n",
        "run_experiment(transformer_model)\n",
        "run_translation(transformer_model, test_iter, max_len=100)\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}